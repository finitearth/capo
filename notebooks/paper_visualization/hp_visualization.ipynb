{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from capo.analysis.utils import (\n",
    "    generate_comparison_table,\n",
    ")\n",
    "from capo.analysis.visualizations import (\n",
    "    plot_population_scores_comparison,\n",
    "    plot_length_score,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"agnews\", \"gsm8k\"]\n",
    "# markes to choose: [\"8\",\"s\",\"D\",\"o\",\"^\",\"p\",\"X\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\n",
    "    \"CAPO_no_lp\",\n",
    "    \"CAPO_gamma_0.01\",\n",
    "    \"CAPO_gamma_0.02\",\n",
    "    \"CAPO_gamma_0.05\",\n",
    "    \"dummy\",\n",
    "    \"dummy\",\n",
    "    \"CAPO_gamma_0.1\",\n",
    "]\n",
    "markers = [\"8\", \"s\", \"d\", \"o\", None, None, \"p\"]\n",
    "labels = [\n",
    "    r\"$\\gamma=0$\",\n",
    "    r\"$\\gamma=0.01$\",\n",
    "    r\"$\\gamma=0.02$\",\n",
    "    r\"$\\gamma=0.05$ (CAPO)\",\n",
    "    \"Dummy\",\n",
    "    \"Dummy\",\n",
    "    r\"$\\gamma=0.1$\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    plot_population_scores_comparison(\n",
    "        dataset,\n",
    "        \"llama\",\n",
    "        hp_runs,\n",
    "        \"mean\",\n",
    "        plot_seeds=False,\n",
    "        plot_stddev=False,\n",
    "        x_col=\"step\",\n",
    "        score_col=\"test_score\",\n",
    "        continuous_colors=True,\n",
    "        markers=markers,\n",
    "        labels=labels,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test score is just slightly better without length penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\",\n",
    "    \"llama\",\n",
    "    hp_runs,\n",
    "    \"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=False,\n",
    "    x_col=\"step\",\n",
    "    score_col=\"prompt_len\",\n",
    "    continuous_colors=True,\n",
    "    markers=markers,\n",
    "    labels=labels,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prompt length is clearly shorter than without the length penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see this also for gsm8k but the effect is smaller here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    plot_length_score(\n",
    "        dataset,\n",
    "        \"llama\",\n",
    "        [\"CAPO_gamma_0.1\", \"nan\", \"nan\", \"CAPO_no_lp\"],\n",
    "        x_col=\"prompt_len\",\n",
    "        score_col=\"test_score\",\n",
    "        log_scale=False,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for agnews we can see that all prompts tend to be clearly longer than for a high length penalty and the best scores also are longer without the penalty\n",
    "- for gsm8k this can only be seen in the extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    DATASETS,\n",
    "    [\"CAPO_no_lp\", \"CAPO_gamma_0.01\", \"CAPO_gamma_0.02\", \"CAPO\", \"CAPO_gamma_0.1\"],\n",
    "    \"llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works very good for agnews, but not for gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- we can perform more steps if we choose a higher length penalty (because the prompts are then also shorter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\"CAPO_pop_6\", \"CAPO_pop_8\", \"CAPO_pop_10\", \"Dummy\", \"CAPO_pop_12\"]\n",
    "markers = [\"8\", \"s\", \"o\", None, \"p\"]\n",
    "labels = [r\"$\\mu=6$\", r\"$\\mu=8$\", r\"$\\mu=10$ (CAPO)\", \"Dummy\", r\"$\\mu=12$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    plot_population_scores_comparison(\n",
    "        dataset,\n",
    "        \"llama\",\n",
    "        hp_runs,\n",
    "        \"mean\",\n",
    "        plot_seeds=False,\n",
    "        plot_stddev=True,\n",
    "        x_col=\"step\",\n",
    "        score_col=\"test_score\",\n",
    "        continuous_colors=True,\n",
    "        markers=markers,\n",
    "        labels=labels,\n",
    "        ncols=3,\n",
    "        figsize=(5.4, 3),\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    DATASETS,\n",
    "    [\"CAPO_pop_6\", \"CAPO_pop_8\", \"CAPO\", \"CAPO_pop_12\"],\n",
    "    \"llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- population size does not have a measurable effect on the performance\n",
    "- higher standard deviation for smaller population sizes\n",
    "- we are quite robust for this hyperparameter (i would not say so? i would say this is a tuning parameter with some influence that is not trivial to choose / depends on the task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Crossovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\"Dummy\", \"Dummy\", \"CAPO_ncrossovers_4\", \"CAPO_ncrossovers_7\", \"CAPO_ncrossovers_10\"]\n",
    "markers = [None, None, \"o\", \"p\", \"d\"]\n",
    "labels = [\"Dummy\", \"Dummy\", r\"$c=4$ (CAPO)\", r\"$c=7$\", r\"$c=10$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    plot_population_scores_comparison(\n",
    "        dataset,\n",
    "        \"llama\",\n",
    "        hp_runs,\n",
    "        \"mean\",\n",
    "        plot_seeds=False,\n",
    "        plot_stddev=True,\n",
    "        x_col=\"step\",\n",
    "        score_col=\"test_score\",\n",
    "        continuous_colors=True,\n",
    "        markers=markers,\n",
    "        labels=labels,\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    DATASETS,\n",
    "    [\"CAPO\", \"CAPO_ncrossovers_7\", \"CAPO_ncrossovers_10\"],\n",
    "    \"llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with less crossovers we can do much more steps (do not have to evaluate to many new prompts)\n",
    "- agnews has a higher variance with 10 crossovers\n",
    "- slightly better performacne for n_crossovers = 7 (might have been a better choice - we are more sensitive to this hyperparameter than to the one we looked at before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hp_runs = [\n",
    "    \"CAPO\",\n",
    "    \"CAPO_no_lp\",\n",
    "    \"CAPO_gamma_0.01\",\n",
    "    \"CAPO_gamma_0.02\",\n",
    "    \"CAPO_gamma_0.1\",\n",
    "    \"CAPO_pop_6\",\n",
    "    \"CAPO_pop_8\",\n",
    "    \"CAPO_pop_12\",\n",
    "    \"CAPO_ncrossovers_7\",\n",
    "    \"CAPO_ncrossovers_10\",\n",
    "]\n",
    "\n",
    "generate_comparison_table(DATASETS, all_hp_runs, \"llama\", score_col=\"test_score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
