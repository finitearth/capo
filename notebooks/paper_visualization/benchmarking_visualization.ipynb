{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from capo.analysis.utils import (\n",
    "    get_results,\n",
    "    generate_comparison_table,\n",
    ")\n",
    "from capo.analysis.visualizations import (\n",
    "    plot_population_members,\n",
    "    plot_population_scores_comparison,\n",
    "    plot_length_score,\n",
    "    plot_performance_profile_curve,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMS = [\"CAPO\", \"OPRO\", \"EvoPromptGA\", \"PromptWizard\", \"Initial\"]\n",
    "OPTIMS_NO_WIZ = [\"CAPO\", \"EvoPromptGA\", \"OPRO\", \"Initial\"]\n",
    "OPTIMS_NO_SING = [\"CAPO\", \"EvoPromptGA\", \"OPRO\"]\n",
    "DATASETS = [\"sst-5\", \"agnews\", \"copa\", \"gsm8k\", \"subj\"]\n",
    "MODELS = [\"llama\", \"qwen\", \"mistral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use mean for aggregation since this better depicts the overall optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_profile_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"sst-5\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    "    figsize=(5.4, 3),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"subj\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> use in main paper next to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"copa\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\",\n",
    "    \"llama\",\n",
    "    [\"CAPO\", \"CAPO_no_racing\", \"EvoPromptGA\"],\n",
    "    colors=[sns.color_palette(\"Dark2\")[0], \"#66D874\", sns.color_palette(\"Dark2\")[2]],\n",
    "    labels=[\"CAPO\", \"CAPO w/o Racing\", \"EvoPromptGA\"],\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=1,\n",
    "    x_col=\"step\",\n",
    "    score_col=\"input_tokens_sum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"subj\",\n",
    "    \"mistral\",\n",
    "    [\"CAPO\", \"\", \"EvoPromptGA\"],\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=1,\n",
    "    x_col=\"step\",\n",
    "    score_col=\"input_tokens_sum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(model=\"qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"sst-5\",\n",
    "    \"qwen\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\",\n",
    "    \"qwen\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"subj\",\n",
    "    \"qwen\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> use in main paper on first side because it is beautiful ðŸ’•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\",\n",
    "    \"qwen\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"copa\",\n",
    "    \"qwen\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(model=\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"sst-5\",\n",
    "    \"llama\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\",\n",
    "    \"llama\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptWizard performs really bad for Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"subj\",\n",
    "    \"llama\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\",\n",
    "    \"llama\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    "    n_seeds_to_plot_std=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\",\n",
    "    \"mistral\",\n",
    "    OPTIMS,\n",
    "    agg=\"mean\",\n",
    "    plot_seeds=False,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidates for main paper\n",
    "- GSM8K (because its most relevant dataset)\n",
    "- Subj using qwen (because it has beautiful curves)\n",
    "\n",
    "Takeaways:\n",
    "- PromptWizard's performance is highly dependend on model used (=> strict templates!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"{model}\")\n",
    "    display(generate_comparison_table(model=model, cutoff_tokens=1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"Model: {model}\")\n",
    "    display(generate_comparison_table(model=model, cutoff_tokens=3_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"{model}\")\n",
    "    display(generate_comparison_table(model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cutoff very early (already at 1_000_000 we outperform the other optimizers in nearly all cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_score(\n",
    "    \"gsm8k\",\n",
    "    \"mistral\",\n",
    "    [\"CAPO\", \"OPRO\", \"EvoPromptGA\", \"PromptWizard\"],\n",
    "    x_col=\"prompt_len\",\n",
    "    score_col=\"test_score\",\n",
    "    log_scale=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> maybe we are cost aware in the sense that we are evaluating the entire \"front\" (EvoPrompt and Opro are very short and Promptwizard very long)\n",
    "\n",
    "- promptwizard has extremly long prompts, that only sometimes can compete with competitors\n",
    "\n",
    "=> interesting for plotting: \n",
    "- subj using qwen or gsm8k using mistral => shows that we have a huge range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Length Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"{model}\")\n",
    "    display(generate_comparison_table(model=model, score_col=\"prompt_len\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_members(\"subj\", \"mistral\", \"CAPO\", x_col=\"step\", score_col=\"test_score\", seeds=[42]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best prompt per dataset, model, optimizer\n",
    "for dataset in [\"subj\"]:\n",
    "    for model in [\"mistral\"]:\n",
    "        for optim in [\"capo\"]:\n",
    "            print(f\"Dataset: {dataset}, Model: {model}, Optimizer: {optim}\")\n",
    "            df = get_results(\n",
    "                dataset=dataset,\n",
    "                model=model,\n",
    "                optim=optim,\n",
    "            )\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "            p, s = df.nlargest(1, \"test_score\")[[\"prompt\", \"test_score\"]].values[0]\n",
    "\n",
    "            print(s)\n",
    "            print(\"'''\")\n",
    "            pp(p)\n",
    "            print(\"'''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "capo can be very repetitive? (SST-5 mistral) potentially the crossover meta prompt has been misinterpreted (merge the two prompts) => however it is performing superior!\n",
    "\n",
    "subj for qwen and llama with capo has a crazy outlier to the top\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
