{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from capo.analysis.utils import (\n",
    "    get_results,\n",
    "    aggregate_results,\n",
    "    get_prompt_scores,\n",
    "    generate_comparison_table,\n",
    ")\n",
    "from capo.analysis.visualizations import (\n",
    "    plot_population_scores,\n",
    "    plot_population_members,\n",
    "    plot_population_scores_comparison,\n",
    "    plot_length_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMS = [\"CAPO\", \"EvoPromptGA\", \"OPRO\", \"PromptWizard\"]\n",
    "DATASETS = [\"sst-5\", \"agnews\", \"copa\", \"gsm8k\", \"subj\"]\n",
    "MODELS = [\"llama\", \"mistral\", \"qwen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    for model in MODELS:\n",
    "        plot_population_scores_comparison(\n",
    "            dataset,\n",
    "            model,\n",
    "            OPTIMS,\n",
    "            agg=\"mean\",\n",
    "            plot_seeds=True,\n",
    "            plot_stddev=True,\n",
    "            x_col=\"input_tokens_cum\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidates for main paper\n",
    "- GSM8K (because its most relevant dataset)\n",
    "- Subj using qwen (because it has beautiful curves)\n",
    "\n",
    "Takeaways:\n",
    "- PromptWizard's performance is highly dependend on model used (=> strict templates!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"Model: {model}\")\n",
    "    display(generate_comparison_table(model=model, cutoff_tokens=1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"Model: {model}\")\n",
    "    display(generate_comparison_table(model=model, cutoff_tokens=3_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"Model: {model}\")\n",
    "    display(generate_comparison_table(model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    for model in MODELS:\n",
    "        plot_length_score(\n",
    "            dataset, model, OPTIMS, x_col=\"prompt_len\", score_col=\"test_score\", log_scale=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> maybe we are cost aware in the sense that we are evaluating the entire \"front\" (EvoPrompt and Opro are very short and Promptwizard very long)\n",
    "\n",
    "- promptwizard has extremly long prompts, that only sometimes can compete with competitors\n",
    "\n",
    "=> interesting for plotting: \n",
    "- subj using qwen or gsm8k using mistral => shows that we have a huge range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best prompt per dataset, model, optimizer\n",
    "for dataset in DATASETS:\n",
    "    for model in MODELS:\n",
    "        for optim in OPTIMS:\n",
    "            print(f\"Dataset: {dataset}, Model: {model}, Optimizer: {optim}\")\n",
    "            df = get_results(\n",
    "                dataset=dataset,\n",
    "                model=model,\n",
    "                optim=optim,\n",
    "                # sort_by=\"test_score\",\n",
    "                # ascending=False,\n",
    "            )\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "            p, s = df.nlargest(1, \"test_score\")[[\"prompt\", \"test_score\"]].values[0]\n",
    "            print(s)\n",
    "            print(\"'''\")\n",
    "            pp(p)\n",
    "            print(\"'''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "capo can be very repetitive? (SST-5 mistral) potentially the crossover meta prompt has been misinterpreted (merge the two prompts) => however it is performing superior!\n",
    "\n",
    "subj for qwen and llama with capo has a crazy outlier to the top\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capo-JzHO-Y9b-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
