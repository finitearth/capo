{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from capo.analysis.utils import (\n",
    "    get_results,\n",
    "    aggregate_results,\n",
    "    get_prompt_scores,\n",
    "    generate_comparison_table,\n",
    ")\n",
    "from capo.analysis.visualizations import (\n",
    "    plot_population_scores,\n",
    "    plot_population_members,\n",
    "    plot_population_scores_comparison,\n",
    "    plot_performance_profile_curve,\n",
    "    plot_train_test_comparison,\n",
    "    plot_length_score,\n",
    ")\n",
    "\n",
    "plot_population_scores(\n",
    "    \"gsm8k\",\n",
    "    \"mistral\",\n",
    "    \"CAPO\",\n",
    "    plot_seeds=True,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    "    color=sns.color_palette(\"Dark2\")[0],\n",
    "    add_title=True,\n",
    "    add_legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_profile_curve(path_prefix=\"../temp/res/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\",\n",
    "    \"mistral\",\n",
    "    optims=[\"CAPO\", \"EvoPromptGA\", \"OPRO\", \"PromptWizard\"],\n",
    "    plot_seeds=True,\n",
    "    plot_stddev=True,\n",
    "    x_col=\"input_tokens_cum\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_comparison(\n",
    "    \"subj\",\n",
    "    \"llama\",\n",
    "    optims=[\"CAPO\", \"EvoPromptGA\", \"OPRO\"],\n",
    "    plot_stddev=False,\n",
    "    x_col=\"input_tokens_cum\",\n",
    "    path_prefix=\"../temp/res/\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_members(\"gsm8k\", \"mistral\", optim=\"CAPO\", x_col=\"step\", seeds=[44]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_score(\n",
    "    \"copa\",\n",
    "    \"mistral\",\n",
    "    [\"CAPO\", \"EvoPromptGA\", \"OPRO\"],\n",
    "    x_col=\"instr_len\",\n",
    "    score_col=\"test_score\",\n",
    "    log_scale=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_runs = [\n",
    "    \"CAPO\",\n",
    "    \"EvoPromptGA\",\n",
    "    \"EvoPromptGA_TA\",\n",
    "    \"CAPO_zero_shot\",\n",
    "    \"CAPO_no_lp\",\n",
    "    \"CAPO_no_racing\",\n",
    "]\n",
    "\n",
    "plot_population_scores_comparison(\n",
    "    \"agnews\", \"llama\", ablation_runs, plot_seeds=False, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\", \"llama\", ablation_runs, plot_seeds=False, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    model=\"llama\",\n",
    "    cutoff_tokens=5_000_000,\n",
    "    optims=ablation_runs,\n",
    "    datasets=[\"agnews\", \"gsm8k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HP search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\n",
    "    \"CAPO\",\n",
    "    \"CAPO_gamma_0.01\",\n",
    "    \"CAPO_gamma_0.05\",\n",
    "    \"CAPO_gamma_0.1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\", \"llama\", hp_runs, \"mean\", plot_seeds=False, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\", \"llama\", hp_runs, \"mean\", plot_seeds=False, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    model=\"llama\",\n",
    "    cutoff_tokens=5_000_000,\n",
    "    optims=hp_runs,\n",
    "    datasets=[\"agnews\", \"gsm8k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_crossovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\n",
    "    \"CAPO\",\n",
    "    \"CAPO_ncrossovers_4\",\n",
    "    \"CAPO_ncrossovers_7\",\n",
    "    \"CAPO_ncrossovers_10\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\", \"llama\", hp_runs, \"mean\", plot_seeds=False, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\", \"llama\", hp_runs, \"mean\", plot_seeds=True, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    model=\"llama\",\n",
    "    cutoff_tokens=5_000_000,\n",
    "    optims=hp_runs,\n",
    "    datasets=[\"agnews\", \"gsm8k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_runs = [\"CAPO\", \"CAPO_pop_6\", \"CAPO_pop_8\", \"CAPO_pop_10\", \"CAPO_pop_12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"agnews\", \"llama\", hp_runs, \"mean\", plot_seeds=True, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_scores_comparison(\n",
    "    \"gsm8k\", \"llama\", hp_runs, \"mean\", plot_seeds=True, plot_stddev=True, x_col=\"input_tokens_cum\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_table(\n",
    "    model=\"llama\",\n",
    "    cutoff_tokens=5_000_000,\n",
    "    optims=hp_runs,\n",
    "    datasets=[\"agnews\", \"gsm8k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unverified plots! ðŸ‘‡ðŸ»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_members(DATASET, MODEL, OPTIM, x_col=\"cum_token\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_convergence_rate(\n",
    "#     dataset, model, optims, agg=\"mean\", score_col=\"test_score\", convergence_threshold=0.9\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Analyze how quickly each algorithm converges to its maximum performance.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     convergence_threshold : float\n",
    "#         The fraction of maximum performance used to define convergence\n",
    "#         (e.g., 0.9 means 90% of the maximum score)\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#     colors = [\"blue\", \"red\"]\n",
    "#     results = []\n",
    "\n",
    "#     for i, optim in enumerate(optims):\n",
    "#         df = get_data(dataset, model, optim)\n",
    "#         df = aggregate_population_scores(df, agg)\n",
    "\n",
    "#         df_avg = df.groupby(\"step\", as_index=False).max(numeric_only=True)\n",
    "\n",
    "#         # Get the maximum score achieved by this optimizer\n",
    "#         max_score = df_avg[score_col].max()\n",
    "#         threshold_score = max_score * convergence_threshold\n",
    "\n",
    "#         # Find when the algorithm first crosses the threshold\n",
    "#         try:\n",
    "#             convergence_point = df_avg[df_avg[score_col] >= threshold_score].iloc[0]\n",
    "#             tokens_to_converge = convergence_point[\"cum_token\"]\n",
    "#             steps_to_converge = convergence_point[\"step\"]\n",
    "\n",
    "#             # Plot vertical line at convergence point\n",
    "#             ax.axvline(x=tokens_to_converge, color=colors[i], linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "#             # Store result\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"optimizer\": optim,\n",
    "#                     \"dataset\": dataset,\n",
    "#                     \"model\": model,\n",
    "#                     \"tokens_to_converge\": tokens_to_converge,\n",
    "#                     \"steps_to_converge\": steps_to_converge,\n",
    "#                     \"max_score\": max_score,\n",
    "#                 }\n",
    "#             )\n",
    "#         except IndexError:\n",
    "#             # Optimizer never reaches the threshold\n",
    "#             print(f\"{optim} never reaches {convergence_threshold*100}% of its maximum performance\")\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"optimizer\": optim,\n",
    "#                     \"dataset\": dataset,\n",
    "#                     \"model\": model,\n",
    "#                     \"tokens_to_converge\": float(\"inf\"),\n",
    "#                     \"steps_to_converge\": float(\"inf\"),\n",
    "#                     \"max_score\": max_score,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         # Plot performance curve\n",
    "#         ax.plot(\n",
    "#             df_avg[\"cum_token\"],\n",
    "#             df_avg[score_col],\n",
    "#             label=f\"{optim} (max: {max_score:.3f})\",\n",
    "#             color=colors[i],\n",
    "#             linewidth=2,\n",
    "#         )\n",
    "\n",
    "#     ax.set_xlabel(\"Cumulative Tokens\")\n",
    "#     ax.set_ylabel(score_col)\n",
    "#     ax.set_title(\n",
    "#         f\"{dataset} - {model} - Convergence Analysis ({convergence_threshold*100}% threshold)\"\n",
    "#     )\n",
    "#     ax.legend()\n",
    "\n",
    "#     # Create a DataFrame with the convergence results\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     return fig, results_df\n",
    "\n",
    "\n",
    "# def plot_convergence_comparison(\n",
    "#     datasets=[\"sst-5\", \"agnews\", \"rte\", \"gsm8k\", \"subj\"],\n",
    "#     optimizers=[\"CAPO\", \"EvoPromptGA\"],\n",
    "#     model=\"llama\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create a bar chart comparing tokens to convergence across datasets.\n",
    "#     \"\"\"\n",
    "#     all_results = []\n",
    "\n",
    "#     for dataset in datasets:\n",
    "#         _, results_df = analyze_convergence_rate(dataset, model, optimizers)\n",
    "#         all_results.append(results_df)\n",
    "\n",
    "#     combined_results = pd.concat(all_results)\n",
    "\n",
    "#     # Create grouped bar chart\n",
    "#     fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "#     # Get unique datasets and optimizers\n",
    "#     unique_datasets = combined_results[\"dataset\"].unique()\n",
    "#     unique_optims = combined_results[\"optimizer\"].unique()\n",
    "\n",
    "#     x = np.arange(len(unique_datasets))\n",
    "#     width = 0.35\n",
    "\n",
    "#     # Create bars for each optimizer\n",
    "#     for i, optim in enumerate(unique_optims):\n",
    "#         optim_data = combined_results[combined_results[\"optimizer\"] == optim]\n",
    "\n",
    "#         # Convert 'inf' to a large but finite number for plotting\n",
    "#         tokens_data = [\n",
    "#             optim_data[optim_data[\"dataset\"] == dataset][\"tokens_to_converge\"].values[0]\n",
    "#             if dataset in optim_data[\"dataset\"].values\n",
    "#             and optim_data[optim_data[\"dataset\"] == dataset][\"tokens_to_converge\"].values[0]\n",
    "#             != float(\"inf\")\n",
    "#             else np.nan\n",
    "#             for dataset in unique_datasets\n",
    "#         ]\n",
    "\n",
    "#         # Plot bars\n",
    "#         ax.bar(x + (i - 0.5) * width, tokens_data, width, label=optim)\n",
    "\n",
    "#     # Customize the plot\n",
    "#     ax.set_xlabel(\"Dataset\")\n",
    "#     ax.set_ylabel(\"Tokens to Convergence\")\n",
    "#     ax.set_title(f\"Tokens Required to Reach 90% of Maximum Performance - {model}\")\n",
    "#     ax.set_xticks(x)\n",
    "#     ax.set_xticklabels(unique_datasets)\n",
    "#     ax.legend()\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Example usage\n",
    "# _, results_df = analyze_convergence_rate(\n",
    "#     \"gsm8k\", \"llama\", [\"CAPO\", \"EvoPromptGA\"], convergence_threshold=0.99\n",
    "# )\n",
    "# plot_convergence_comparison();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_compute_efficiency(\n",
    "#     dataset, model, optims, cost_per_1k_tokens=0.005, score_col=\"test_score\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Analyze the cost-effectiveness of each algorithm.\n",
    "\n",
    "#     This assumes a simple cost model based on token usage.\n",
    "#     Adapt the cost_per_1k_tokens parameter based on your model costs.\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#     colors = [\"blue\", \"red\"]\n",
    "\n",
    "#     for i, optim in enumerate(optims):\n",
    "#         df = get_data(dataset, model, optim)\n",
    "#         df = aggregate_population_scores(df, \"mean\")\n",
    "\n",
    "#         df_avg = df.groupby(\"step\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "#         # Calculate cost\n",
    "#         df_avg[\"cost\"] = df_avg[\"cum_token\"] * cost_per_1k_tokens / 1000\n",
    "\n",
    "#         # Calculate cost effectiveness as score per dollar\n",
    "#         df_avg[\"cost_effectiveness\"] = df_avg[score_col] / df_avg[\"cost\"]\n",
    "\n",
    "#         # Plot cost effectiveness\n",
    "#         ax.plot(\n",
    "#             df_avg[\"cum_token\"],\n",
    "#             df_avg[\"cost_effectiveness\"],\n",
    "#             label=f\"{optim}\",\n",
    "#             color=colors[i],\n",
    "#             linewidth=2,\n",
    "#         )\n",
    "\n",
    "#     ax.set_xlabel(\"Cumulative Tokens\")\n",
    "#     ax.set_ylabel(\"Score per Dollar\")\n",
    "#     ax.set_title(f\"{dataset} - {model} - Cost Effectiveness\")\n",
    "#     ax.legend()\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# def plot_cost_performance_tradeoff(\n",
    "#     datasets=[\"sst-5\", \"agnews\", \"rte\", \"gsm8k\", \"subj\"],\n",
    "#     optimizers=[\"CAPO\", \"EvoPromptGA\"],\n",
    "#     model=\"llama\",\n",
    "#     cutoff_tokens=5_000_000,\n",
    "#     cost_per_1k_tokens=0.005,\n",
    "#     agg=\"mean\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create a scatter plot showing the cost-performance tradeoff across datasets.\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "#     # Define markers for datasets\n",
    "#     markers = [\"o\", \"s\", \"^\", \"D\", \"p\"]  # circle, square, triangle, diamond, pentagon\n",
    "\n",
    "#     # Store results for annotation\n",
    "#     results = []\n",
    "\n",
    "#     # Loop through optimizers and datasets\n",
    "#     for i, optim in enumerate(optimizers):\n",
    "#         costs = []\n",
    "#         scores = []\n",
    "#         dataset_labels = []\n",
    "\n",
    "#         for j, dataset in enumerate(datasets):\n",
    "#             try:\n",
    "#                 df = get_data(dataset, model, optim)\n",
    "#                 df = aggregate_population_scores(df, agg)\n",
    "\n",
    "#                 # Get values at token cutoff\n",
    "#                 max_steps = []\n",
    "#                 for seed in df.seed.unique():\n",
    "#                     df_seed = df[df.seed == seed]\n",
    "#                     last_step = df_seed.loc[df_seed[\"cum_token\"] < cutoff_tokens, \"step\"].max()\n",
    "#                     if pd.isna(last_step):\n",
    "#                         continue\n",
    "#                     max_steps.append(df_seed[df_seed[\"step\"] == last_step])\n",
    "\n",
    "#                 if max_steps:\n",
    "#                     final_df = pd.concat(max_steps)\n",
    "#                     avg_score = final_df[\"test_score\"].mean() * 100  # Convert to percentage\n",
    "#                     avg_tokens = final_df[\"cum_token\"].mean()\n",
    "#                     cost = avg_tokens * cost_per_1k_tokens / 1000  # Cost in dollars\n",
    "\n",
    "#                     costs.append(cost)\n",
    "#                     scores.append(avg_score)\n",
    "#                     dataset_labels.append(dataset)\n",
    "\n",
    "#                     # Store result\n",
    "#                     results.append(\n",
    "#                         {\"dataset\": dataset, \"optimizer\": optim, \"cost\": cost, \"score\": avg_score}\n",
    "#                     )\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {dataset} - {optim}: {e}\")\n",
    "\n",
    "#         # Plot scatter points\n",
    "#         for j, (cost, score, dataset) in enumerate(zip(costs, scores, dataset_labels)):\n",
    "#             ax.scatter(\n",
    "#                 cost,\n",
    "#                 score,\n",
    "#                 marker=markers[j % len(markers)],\n",
    "#                 s=100,\n",
    "#                 color=f\"C{i}\",\n",
    "#                 label=f\"{optim} - {dataset}\",\n",
    "#             )\n",
    "\n",
    "#     # Add labels\n",
    "#     ax.set_xlabel(\"Estimated Cost ($)\")\n",
    "#     ax.set_ylabel(\"Test Score (%)\")\n",
    "#     ax.set_title(f\"Cost-Performance Tradeoff - {model}\")\n",
    "\n",
    "#     # Add legend outside the plot\n",
    "#     ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "#     # Adjust layout to make room for the legend\n",
    "#     plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "#     # Create results dataframe\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     return fig, results_df\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# plot_compute_efficiency(\"gsm8k\", \"llama\", [\"CAPO\", \"EvoPromptGA\"])\n",
    "# plot_cost_performance_tradeoff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_learning_curve(dataset, model, optims, score_col=\"test_score\"):\n",
    "#     \"\"\"\n",
    "#     Fit a learning curve model to estimate asymptotic performance.\n",
    "#     Uses an exponential approach model: s(t) = a - b*exp(-c*t)\n",
    "#     where:\n",
    "#     - a is the asymptotic performance\n",
    "#     - b is the scaling factor\n",
    "#     - c is the learning rate\n",
    "#     - t is the number of tokens\n",
    "#     \"\"\"\n",
    "#     from scipy.optimize import curve_fit\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#     colors = [\"blue\", \"red\"]\n",
    "#     results = []\n",
    "\n",
    "#     # Define learning curve function\n",
    "#     def learning_curve(x, a, b, c):\n",
    "#         return a - b * np.exp(-c * x)\n",
    "\n",
    "#     for i, optim in enumerate(optims):\n",
    "#         df = get_data(dataset, model, optim)\n",
    "#         df = aggregate_population_scores(df, \"mean\")\n",
    "\n",
    "#         df_avg = df.groupby(\"step\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "#         # Prepare data for fitting\n",
    "#         x_data = df_avg[\"cum_token\"].values\n",
    "#         y_data = df_avg[score_col].values\n",
    "\n",
    "#         try:\n",
    "#             # Initial parameter guess\n",
    "#             p0 = [max(y_data) * 1.1, max(y_data) - min(y_data), 1e-6]\n",
    "\n",
    "#             # Fit curve\n",
    "#             popt, pcov = curve_fit(learning_curve, x_data, y_data, p0=p0, maxfev=10000)\n",
    "\n",
    "#             # Extract parameters\n",
    "#             a, b, c = popt\n",
    "\n",
    "#             # Calculate extrapolated performance\n",
    "#             x_extrapolated = np.linspace(min(x_data), max(x_data) * 2, 100)\n",
    "#             y_extrapolated = learning_curve(x_extrapolated, a, b, c)\n",
    "\n",
    "#             # Plot actual data\n",
    "#             ax.plot(x_data, y_data, \"o\", color=colors[i], alpha=0.5, label=f\"{optim} (actual)\")\n",
    "\n",
    "#             # Plot fitted curve\n",
    "#             ax.plot(\n",
    "#                 x_extrapolated,\n",
    "#                 y_extrapolated,\n",
    "#                 \"-\",\n",
    "#                 color=colors[i],\n",
    "#                 label=f\"{optim} (fitted, asymptote={a:.3f})\",\n",
    "#             )\n",
    "\n",
    "#             # Store parameters\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"dataset\": dataset,\n",
    "#                     \"model\": model,\n",
    "#                     \"optimizer\": optim,\n",
    "#                     \"asymptotic_performance\": a,\n",
    "#                     \"scaling_factor\": b,\n",
    "#                     \"learning_rate\": c,\n",
    "#                     \"tokens_to_90pct\": -np.log(0.1) / c if c > 0 else float(\"inf\"),\n",
    "#                     \"tokens_to_95pct\": -np.log(0.05) / c if c > 0 else float(\"inf\"),\n",
    "#                     \"tokens_to_99pct\": -np.log(0.01) / c if c > 0 else float(\"inf\"),\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fitting curve for {optim}: {e}\")\n",
    "\n",
    "#     ax.set_xlabel(\"Cumulative Tokens\")\n",
    "#     ax.set_ylabel(score_col)\n",
    "#     ax.set_title(f\"{dataset} - {model} - Learning Curve Extrapolation\")\n",
    "#     ax.legend()\n",
    "\n",
    "#     # Create results dataframe\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     return fig, results_df\n",
    "\n",
    "\n",
    "# def plot_asymptotic_comparison(\n",
    "#     datasets=[\"sst-5\", \"agnews\", \"rte\", \"gsm8k\", \"subj\"],\n",
    "#     optimizers=[\"CAPO\", \"EvoPromptGA\"],\n",
    "#     model=\"llama\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create a bar chart comparing the estimated asymptotic performance across datasets.\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "#     # Store all results\n",
    "#     all_results = []\n",
    "\n",
    "#     for dataset in datasets:\n",
    "#         try:\n",
    "#             _, results_df = fit_learning_curve(dataset, model, optimizers)\n",
    "#             all_results.append(results_df)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {dataset}: {e}\")\n",
    "\n",
    "#     # Combine results\n",
    "#     if all_results:\n",
    "#         combined_results = pd.concat(all_results)\n",
    "\n",
    "#         # Set up grouped bar chart\n",
    "#         unique_datasets = combined_results[\"dataset\"].unique()\n",
    "#         unique_optims = combined_results[\"optimizer\"].unique()\n",
    "\n",
    "#         x = np.arange(len(unique_datasets))\n",
    "#         width = 0.35\n",
    "\n",
    "#         # Create bars for each optimizer\n",
    "#         for i, optim in enumerate(unique_optims):\n",
    "#             optim_data = combined_results[combined_results[\"optimizer\"] == optim]\n",
    "\n",
    "#             asymptotic_data = [\n",
    "#                 optim_data[optim_data[\"dataset\"] == dataset][\"asymptotic_performance\"].values[0]\n",
    "#                 * 100\n",
    "#                 if dataset in optim_data[\"dataset\"].values\n",
    "#                 else np.nan\n",
    "#                 for dataset in unique_datasets\n",
    "#             ]\n",
    "\n",
    "#             # Plot bars\n",
    "#             ax.bar(x + (i - 0.5) * width, asymptotic_data, width, label=optim)\n",
    "\n",
    "#         # Customize plot\n",
    "#         ax.set_xlabel(\"Dataset\")\n",
    "#         ax.set_ylabel(\"Asymptotic Performance (%)\")\n",
    "#         ax.set_title(f\"Predicted Maximum Performance - {model}\")\n",
    "#         ax.set_xticks(x)\n",
    "#         ax.set_xticklabels(unique_datasets)\n",
    "#         ax.legend()\n",
    "#     else:\n",
    "#         ax.text(0.5, 0.5, \"No results available\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# fit_learning_curve(\"gsm8k\", \"llama\", [\"CAPO\", \"EvoPromptGA\"])\n",
    "# plot_asymptotic_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_prompt_diversity(dataset, model, optimizers, seed=None, max_steps=None):\n",
    "#     \"\"\"\n",
    "#     Analyze the diversity of prompts generated during optimization.\n",
    "\n",
    "#     Diversity can be measured in multiple ways:\n",
    "#     1. Lexical diversity (using metrics like TTR - Type-Token Ratio)\n",
    "#     2. Semantic diversity (using embeddings and clustering)\n",
    "#     3. Length distribution\n",
    "#     4. Instruction pattern changes\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     dataset : str\n",
    "#         The dataset name\n",
    "#     model : str\n",
    "#         The model name\n",
    "#     optimizers : list\n",
    "#         List of optimizer names to compare\n",
    "#     seed : int, optional\n",
    "#         If provided, analyze only this seed. Otherwise, analyze all seeds.\n",
    "#     max_steps : int, optional\n",
    "#         If provided, limit analysis to this many steps\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "#     import numpy as np\n",
    "#     from collections import Counter\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#     from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     for optimizer in optimizers:\n",
    "#         # Get data using the existing function\n",
    "#         df = get_data(dataset, model, optimizer)\n",
    "\n",
    "#         # Filter for specific seed if requested\n",
    "#         if seed is not None:\n",
    "#             df = df[df.seed == seed]\n",
    "\n",
    "#         if df.empty:\n",
    "#             print(f\"No data found for {optimizer}\")\n",
    "#             continue\n",
    "\n",
    "#         # Check if 'prompt' column exists\n",
    "#         if \"prompt\" not in df.columns:\n",
    "#             print(f\"No 'prompt' column found for {optimizer}\")\n",
    "#             continue\n",
    "\n",
    "#         # Group by seed and step to ensure we have one prompt per step\n",
    "#         df_prompts = df.groupby([\"seed\", \"step\"]).first().reset_index()\n",
    "\n",
    "#         # Limit steps if specified\n",
    "#         if max_steps is not None and max_steps < len(df_prompts):\n",
    "#             step_indices = list(range(0, len(df_prompts), len(df_prompts) // max_steps))\n",
    "#             df_prompts = df_prompts.iloc[step_indices]\n",
    "\n",
    "#         # Extract prompts, steps, and seeds\n",
    "#         prompts = df_prompts[\"prompt\"].tolist()\n",
    "#         steps = df_prompts[\"step\"].tolist()\n",
    "#         seeds = df_prompts[\"seed\"].tolist()\n",
    "\n",
    "#         if not prompts:\n",
    "#             print(f\"No prompts found for {optimizer}\")\n",
    "#             continue\n",
    "\n",
    "#         # Calculate lexical diversity (TTR) for each prompt\n",
    "#         def calculate_ttr(text):\n",
    "#             tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "#             if not tokens:\n",
    "#                 return 0\n",
    "#             return len(set(tokens)) / len(tokens)\n",
    "\n",
    "#         ttrs = [calculate_ttr(p) for p in prompts]\n",
    "\n",
    "#         # Calculate prompt lengths\n",
    "#         lengths = [len(p) for p in prompts]\n",
    "\n",
    "#         # Calculate similarity matrix using TF-IDF\n",
    "#         vectorizer = TfidfVectorizer()\n",
    "#         try:\n",
    "#             tfidf_matrix = vectorizer.fit_transform(prompts)\n",
    "#             similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "#             # Calculate average similarity to previous prompt\n",
    "#             prev_similarities = []\n",
    "#             for i in range(1, len(prompts)):\n",
    "#                 prev_similarities.append(similarity_matrix[i, i - 1])\n",
    "\n",
    "#             # Calculate average similarity to all other prompts\n",
    "#             avg_similarities = []\n",
    "#             for i in range(len(prompts)):\n",
    "#                 # Exclude self-similarity (which is always 1.0)\n",
    "#                 other_similarities = [\n",
    "#                     similarity_matrix[i, j] for j in range(len(prompts)) if j != i\n",
    "#                 ]\n",
    "#                 avg_similarities.append(np.mean(other_similarities))\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error calculating similarities for {optimizer}: {e}\")\n",
    "#             prev_similarities = []\n",
    "#             avg_similarities = [0] * len(prompts)\n",
    "\n",
    "#         # Store results\n",
    "#         for i, (step, ttr, length, avg_sim) in enumerate(\n",
    "#             zip(steps, ttrs, lengths, avg_similarities)\n",
    "#         ):\n",
    "#             prev_sim = prev_similarities[i - 1] if i > 0 else None\n",
    "\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"dataset\": dataset,\n",
    "#                     \"model\": model,\n",
    "#                     \"optimizer\": optimizer,\n",
    "#                     \"step\": step,\n",
    "#                     \"ttr\": ttr,\n",
    "#                     \"length\": length,\n",
    "#                     \"avg_similarity\": avg_sim,\n",
    "#                     \"prev_similarity\": prev_sim,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     # Create visualizations\n",
    "#     if not results_df.empty:\n",
    "#         # Create figure with subplots\n",
    "#         fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "#         # Plot 1: Lexical diversity over time\n",
    "#         for optimizer in optimizers:\n",
    "#             optim_data = results_df[results_df[\"optimizer\"] == optimizer]\n",
    "#             if not optim_data.empty:\n",
    "#                 axes[0, 0].plot(optim_data[\"step\"], optim_data[\"ttr\"], marker=\"o\", label=optimizer)\n",
    "\n",
    "#         axes[0, 0].set_xlabel(\"Step\")\n",
    "#         axes[0, 0].set_ylabel(\"Type-Token Ratio\")\n",
    "#         axes[0, 0].set_title(\"Lexical Diversity Over Time\")\n",
    "#         axes[0, 0].legend()\n",
    "\n",
    "#         # Plot 2: Prompt length over time\n",
    "#         for optimizer in optimizers:\n",
    "#             optim_data = results_df[results_df[\"optimizer\"] == optimizer]\n",
    "#             if not optim_data.empty:\n",
    "#                 axes[0, 1].plot(\n",
    "#                     optim_data[\"step\"], optim_data[\"length\"], marker=\"o\", label=optimizer\n",
    "#                 )\n",
    "\n",
    "#         axes[0, 1].set_xlabel(\"Step\")\n",
    "#         axes[0, 1].set_ylabel(\"Prompt Length (chars)\")\n",
    "#         axes[0, 1].set_title(\"Prompt Length Over Time\")\n",
    "#         axes[0, 1].legend()\n",
    "\n",
    "#         # Plot 3: Average similarity to all prompts\n",
    "#         for optimizer in optimizers:\n",
    "#             optim_data = results_df[results_df[\"optimizer\"] == optimizer]\n",
    "#             if not optim_data.empty:\n",
    "#                 axes[1, 0].plot(\n",
    "#                     optim_data[\"step\"], optim_data[\"avg_similarity\"], marker=\"o\", label=optimizer\n",
    "#                 )\n",
    "\n",
    "#         axes[1, 0].set_xlabel(\"Step\")\n",
    "#         axes[1, 0].set_ylabel(\"Average Similarity\")\n",
    "#         axes[1, 0].set_title(\"Prompt Similarity (to all prompts)\")\n",
    "#         axes[1, 0].legend()\n",
    "\n",
    "#         # Plot 4: Similarity to previous prompt\n",
    "#         for optimizer in optimizers:\n",
    "#             optim_data = results_df[results_df[\"optimizer\"] == optimizer].dropna(\n",
    "#                 subset=[\"prev_similarity\"]\n",
    "#             )\n",
    "#             if not optim_data.empty:\n",
    "#                 axes[1, 1].plot(\n",
    "#                     optim_data[\"step\"], optim_data[\"prev_similarity\"], marker=\"o\", label=optimizer\n",
    "#                 )\n",
    "\n",
    "#         axes[1, 1].set_xlabel(\"Step\")\n",
    "#         axes[1, 1].set_ylabel(\"Similarity to Previous\")\n",
    "#         axes[1, 1].set_title(\"Sequential Prompt Changes\")\n",
    "#         axes[1, 1].legend()\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.suptitle(f\"Prompt Diversity Analysis: {dataset} - {model}\", y=1.02)\n",
    "\n",
    "#         return fig, results_df\n",
    "#     else:\n",
    "#         print(\"No data available for diversity analysis\")\n",
    "#         return None, None\n",
    "\n",
    "\n",
    "# def visualize_prompt_space_evolution(\n",
    "#     dataset, model, optimizer, seed=42, embedding_method=\"tfidf\", dim_reduction=\"tsne\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Visualize how prompts evolve in the semantic space using dimensionality reduction.\n",
    "\n",
    "#     Args:\n",
    "#         dataset: The dataset name\n",
    "#         model: The model name\n",
    "#         optimizer: The optimizer name\n",
    "#         seed: The seed to analyze\n",
    "#         embedding_method: 'tfidf' or 'bert' for embedding calculation\n",
    "#         dim_reduction: 'tsne' or 'pca' for dimensionality reduction\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#     from sklearn.decomposition import PCA\n",
    "#     from sklearn.manifold import TSNE\n",
    "\n",
    "#     # Get data using the existing function\n",
    "#     df = get_data(dataset, model, optimizer)\n",
    "\n",
    "#     # Filter for the specific seed\n",
    "#     df = df[df.seed == seed]\n",
    "\n",
    "#     if df.empty:\n",
    "#         print(f\"No data found for {optimizer} with seed {seed}\")\n",
    "#         return None\n",
    "\n",
    "#     # Check if 'prompt' column exists\n",
    "#     if \"prompt\" not in df.columns:\n",
    "#         print(f\"No 'prompt' column found for {optimizer}\")\n",
    "#         return None\n",
    "\n",
    "#     # Group by step to ensure we have one prompt per step\n",
    "#     df_prompts = df.groupby(\"step\").first().reset_index()\n",
    "\n",
    "#     # Sort by step\n",
    "#     df_prompts = df_prompts.sort_values(\"step\")\n",
    "\n",
    "#     # Extract prompts and steps\n",
    "#     prompts = df_prompts[\"prompt\"].tolist()\n",
    "#     steps = df_prompts[\"step\"].tolist()\n",
    "\n",
    "#     if not prompts:\n",
    "#         print(f\"No prompts found for {optimizer} with seed {seed}\")\n",
    "#         return None\n",
    "\n",
    "#     # Create embeddings\n",
    "#     if embedding_method == \"tfidf\":\n",
    "#         vectorizer = TfidfVectorizer(max_features=100)\n",
    "#         embeddings = vectorizer.fit_transform(prompts).toarray()\n",
    "#     elif embedding_method == \"bert\":\n",
    "#         # You would need a sentence transformer library for this\n",
    "#         # For example with sentence-transformers:\n",
    "#         # from sentence_transformers import SentenceTransformer\n",
    "#         # model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#         # embeddings = model.encode(prompts)\n",
    "#         print(\"BERT embeddings not implemented in this function\")\n",
    "#         return None\n",
    "\n",
    "#     # Apply dimensionality reduction\n",
    "#     if embeddings.shape[0] < 3:\n",
    "#         print(\"Too few samples for meaningful dimensionality reduction\")\n",
    "#         return None\n",
    "\n",
    "#     if dim_reduction == \"tsne\":\n",
    "#         reducer = TSNE(n_components=2, random_state=42)\n",
    "#     elif dim_reduction == \"pca\":\n",
    "#         reducer = PCA(n_components=2, random_state=42)\n",
    "\n",
    "#     reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "#     # Create visualization\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "#     # Create colormap for steps\n",
    "#     step_colors = plt.cm.viridis(np.linspace(0, 1, len(steps)))\n",
    "\n",
    "#     # Plot points\n",
    "#     scatter = ax.scatter(\n",
    "#         reduced_embeddings[:, 0],\n",
    "#         reduced_embeddings[:, 1],\n",
    "#         c=steps,\n",
    "#         cmap=\"viridis\",\n",
    "#         s=100,\n",
    "#         alpha=0.7,\n",
    "#     )\n",
    "\n",
    "#     # Add step labels\n",
    "#     for i, (x, y, step) in enumerate(\n",
    "#         zip(reduced_embeddings[:, 0], reduced_embeddings[:, 1], steps)\n",
    "#     ):\n",
    "#         ax.annotate(f\"{step}\", (x, y), xytext=(5, 5), textcoords=\"offset points\")\n",
    "\n",
    "#     # Connect points with lines to show evolution\n",
    "#     ax.plot(reduced_embeddings[:, 0], reduced_embeddings[:, 1], \"-\", color=\"gray\", alpha=0.5)\n",
    "\n",
    "#     # Add colorbar\n",
    "#     cbar = plt.colorbar(scatter)\n",
    "#     cbar.set_label(\"Step\")\n",
    "\n",
    "#     # Customize the plot\n",
    "#     ax.set_title(f\"Prompt Space Evolution: {dataset} - {model} - {optimizer}\")\n",
    "#     ax.set_xlabel(f\"{dim_reduction.upper()} Dimension 1\")\n",
    "#     ax.set_ylabel(f\"{dim_reduction.upper()} Dimension 2\")\n",
    "\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run\n",
    "# analyze_prompt_diversity(\"gsm8k\", \"llama\", [\"CAPO\", \"EvoPromptGA\"], max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_optimization_trajectory(dataset, model, optimizers, seed=42, score_col=\"test_score\"):\n",
    "#     \"\"\"\n",
    "#     Visualize the optimization trajectory as a 2D or 3D path.\n",
    "\n",
    "#     This shows how the optimization navigates through the performance landscape,\n",
    "#     revealing exploration patterns, local optima, and search strategies.\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from mpl_toolkits.mplot3d import Axes3D\n",
    "#     import numpy as np\n",
    "\n",
    "#     # Create figures\n",
    "#     fig1, ax1 = plt.subplots(figsize=(10, 6))  # Score vs. Step\n",
    "#     fig2, ax2 = plt.subplots(figsize=(10, 6))  # Delta Score vs. Current Score\n",
    "#     fig3 = plt.figure(figsize=(12, 10))\n",
    "#     ax3 = fig3.add_subplot(111, projection=\"3d\")  # 3D trajectory\n",
    "\n",
    "#     colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
    "\n",
    "#     for i, optimizer in enumerate(optimizers):\n",
    "#         # Get the data\n",
    "#         df = get_data(dataset, model, optimizer)\n",
    "\n",
    "#         # Filter for the specific seed\n",
    "#         df_seed = df[df.seed == seed]\n",
    "\n",
    "#         if df_seed.empty:\n",
    "#             print(f\"No data for seed {seed} with optimizer {optimizer}\")\n",
    "#             continue\n",
    "\n",
    "#         # Sort by step\n",
    "#         df_seed = df_seed.sort_values(\"step\")\n",
    "\n",
    "#         # Extract key metrics\n",
    "#         steps = df_seed[\"step\"].values\n",
    "#         scores = df_seed[score_col].values\n",
    "#         tokens = df_seed[\"cum_token\"].values\n",
    "\n",
    "#         # Calculate score changes (deltas)\n",
    "#         score_deltas = np.diff(scores, prepend=scores[0])\n",
    "\n",
    "#         # Plot 1: Score vs Step (basic learning curve)\n",
    "#         ax1.plot(steps, scores, marker=\"o\", color=colors[i % len(colors)], label=optimizer)\n",
    "\n",
    "#         # Plot 2: Delta Score vs Current Score (phase space)\n",
    "#         ax2.scatter(\n",
    "#             scores[:-1], score_deltas[1:], color=colors[i % len(colors)], label=optimizer, alpha=0.7\n",
    "#         )\n",
    "#         ax2.plot(scores[:-1], score_deltas[1:], color=colors[i % len(colors)], alpha=0.4)\n",
    "\n",
    "#         # Add arrows to show direction\n",
    "#         for j in range(0, len(scores) - 1, max(1, len(scores) // 10)):  # Add arrows at intervals\n",
    "#             ax2.annotate(\n",
    "#                 \"\",\n",
    "#                 xy=(\n",
    "#                     scores[j + 1],\n",
    "#                     score_deltas[j + 2] if j + 2 < len(score_deltas) else score_deltas[j + 1],\n",
    "#                 ),\n",
    "#                 xytext=(scores[j], score_deltas[j + 1]),\n",
    "#                 arrowprops=dict(arrowstyle=\"->\", color=colors[i % len(colors)], lw=1.5),\n",
    "#             )\n",
    "\n",
    "#         # Plot 3: 3D trajectory (step, score, delta)\n",
    "#         ax3.plot(\n",
    "#             steps[:-1],\n",
    "#             scores[:-1],\n",
    "#             score_deltas[1:],\n",
    "#             color=colors[i % len(colors)],\n",
    "#             label=optimizer,\n",
    "#         )\n",
    "#         ax3.scatter(\n",
    "#             steps[:-1], scores[:-1], score_deltas[1:], color=colors[i % len(colors)], alpha=0.7\n",
    "#         )\n",
    "\n",
    "#     # Customize the plots\n",
    "#     ax1.set_xlabel(\"Step\")\n",
    "#     ax1.set_ylabel(score_col)\n",
    "#     ax1.set_title(f\"Learning Curve: {dataset} - {model} - Seed {seed}\")\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     ax2.set_xlabel(\"Score\")\n",
    "#     ax2.set_ylabel(\"Score Delta (Change)\")\n",
    "#     ax2.set_title(f\"Phase Space: {dataset} - {model} - Seed {seed}\")\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "#     ax2.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)  # Zero line\n",
    "\n",
    "#     ax3.set_xlabel(\"Step\")\n",
    "#     ax3.set_ylabel(\"Score\")\n",
    "#     ax3.set_zlabel(\"Score Delta\")\n",
    "#     ax3.set_title(f\"3D Trajectory: {dataset} - {model} - Seed {seed}\")\n",
    "#     ax3.legend()\n",
    "\n",
    "#     return [fig1, fig2, fig3]\n",
    "\n",
    "\n",
    "# def analyze_search_strategy(dataset, model, optimizer, seed=42, score_col=\"test_score\"):\n",
    "#     \"\"\"\n",
    "#     Analyze the search strategy by examining patterns in score changes.\n",
    "\n",
    "#     This reveals insights about:\n",
    "#     - Exploitation vs. exploration balance\n",
    "#     - Local optima navigation\n",
    "#     - Learning rate adaptation\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import numpy as np\n",
    "#     from scipy.signal import find_peaks\n",
    "\n",
    "#     # Get the data\n",
    "#     df = get_data(dataset, model, optimizer)\n",
    "\n",
    "#     # Filter for the specific seed\n",
    "#     df_seed = df[df.seed == seed]\n",
    "\n",
    "#     if df_seed.empty:\n",
    "#         print(f\"No data for seed {seed} with optimizer {optimizer}\")\n",
    "#         return None\n",
    "\n",
    "#     # Sort by step\n",
    "#     df_seed = df_seed.sort_values(\"step\")\n",
    "\n",
    "#     # Extract key metrics\n",
    "#     steps = df_seed[\"step\"].values\n",
    "#     scores = df_seed[score_col].values\n",
    "#     tokens = df_seed[\"cum_token\"].values\n",
    "\n",
    "#     # Calculate score changes\n",
    "#     deltas = np.diff(scores, prepend=scores[0])\n",
    "#     deltas = deltas[1:]  # Remove the first entry (which is just the initial score)\n",
    "\n",
    "#     # Analyze sign changes (direction changes in optimization)\n",
    "#     sign_changes = np.diff(np.signbit(deltas)).sum()\n",
    "\n",
    "#     # Identify peaks (local maxima)\n",
    "#     peaks, _ = find_peaks(scores)\n",
    "\n",
    "#     # Identify valleys (local minima)\n",
    "#     valleys, _ = find_peaks(-scores)\n",
    "\n",
    "#     # Create figure with multiple subplots\n",
    "#     fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "#     # Plot 1: Score over steps with peaks and valleys\n",
    "#     axes[0].plot(steps, scores, marker=\"o\", color=\"blue\")\n",
    "#     axes[0].scatter(\n",
    "#         steps[peaks], scores[peaks], color=\"green\", s=100, marker=\"^\", label=\"Local Maxima\"\n",
    "#     )\n",
    "#     axes[0].scatter(\n",
    "#         steps[valleys], scores[valleys], color=\"red\", s=100, marker=\"v\", label=\"Local Minima\"\n",
    "#     )\n",
    "#     axes[0].set_ylabel(score_col)\n",
    "#     axes[0].set_title(\n",
    "#         f\"Optimization Landscape Navigation: {dataset} - {model} - {optimizer} - Seed {seed}\"\n",
    "#     )\n",
    "#     axes[0].legend()\n",
    "#     axes[0].grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     # Plot 2: Score deltas (first derivative)\n",
    "#     axes[1].plot(steps[1:], deltas, marker=\"o\", color=\"purple\")\n",
    "#     axes[1].axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
    "#     axes[1].set_ylabel(\"Score Delta\")\n",
    "#     axes[1].set_title(\"Change in Score (First Derivative)\")\n",
    "#     axes[1].grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     # Plot 3: Second derivative (acceleration/deceleration in learning)\n",
    "#     second_deriv = np.diff(deltas, prepend=deltas[0])\n",
    "#     axes[2].plot(steps[1:], second_deriv, marker=\"o\", color=\"orange\")\n",
    "#     axes[2].axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
    "#     axes[2].set_ylabel(\"Score Acceleration\")\n",
    "#     axes[2].set_xlabel(\"Step\")\n",
    "#     axes[2].set_title(\"Acceleration in Learning (Second Derivative)\")\n",
    "#     axes[2].grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     # Add annotations with statistics\n",
    "#     plt.figtext(\n",
    "#         0.02,\n",
    "#         0.02,\n",
    "#         f\"Search Analysis:\\n\"\n",
    "#         f\"- Direction changes: {sign_changes}\\n\"\n",
    "#         f\"- Local maxima: {len(peaks)}\\n\"\n",
    "#         f\"- Local minima: {len(valleys)}\\n\"\n",
    "#         f\"- Max improvement in single step: {deltas.max():.4f}\\n\"\n",
    "#         f\"- Max decline in single step: {deltas.min():.4f}\\n\"\n",
    "#         f\"- Avg positive step: {deltas[deltas > 0].mean() if any(deltas > 0) else 0:.4f}\\n\"\n",
    "#         f\"- Avg negative step: {deltas[deltas < 0].mean() if any(deltas < 0) else 0:.4f}\",\n",
    "#         fontsize=12,\n",
    "#         bbox=dict(facecolor=\"white\", alpha=0.8),\n",
    "#     )\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(bottom=0.15)  # Make room for annotations\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# def compare_optimization_strategies(dataset, model, optimizers, seeds=None, score_col=\"test_score\"):\n",
    "#     \"\"\"\n",
    "#     Compare optimization strategies across algorithms and seeds by analyzing\n",
    "#     the distribution of improvement jumps and exploration patterns.\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import numpy as np\n",
    "#     import seaborn as sns\n",
    "\n",
    "#     if seeds is None:\n",
    "#         # Default to seeds 42, 43, 44 if no seeds specified\n",
    "#         seeds = [42, 43, 44]\n",
    "\n",
    "#     # Store all delta distributions\n",
    "#     all_deltas = {optimizer: [] for optimizer in optimizers}\n",
    "\n",
    "#     # Collect data for all optimizers and seeds\n",
    "#     for optimizer in optimizers:\n",
    "#         for seed in seeds:\n",
    "#             # Get the data\n",
    "#             df = get_data(dataset, model, optimizer)\n",
    "\n",
    "#             # Filter for the specific seed\n",
    "#             df_seed = df[df.seed == seed]\n",
    "\n",
    "#             if df_seed.empty:\n",
    "#                 print(f\"No data for seed {seed} with optimizer {optimizer}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Sort by step\n",
    "#             df_seed = df_seed.sort_values(\"step\")\n",
    "\n",
    "#             # Extract scores\n",
    "#             scores = df_seed[score_col].values\n",
    "\n",
    "#             # Calculate score deltas\n",
    "#             deltas = np.diff(scores)\n",
    "\n",
    "#             # Add to collection\n",
    "#             all_deltas[optimizer].extend(deltas)\n",
    "\n",
    "#     # Create figure with subplots\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "#     # Plot 1: Distribution of score deltas (violin plot)\n",
    "#     delta_data = []\n",
    "#     for optimizer, deltas in all_deltas.items():\n",
    "#         for delta in deltas:\n",
    "#             delta_data.append({\"optimizer\": optimizer, \"delta\": delta})\n",
    "\n",
    "#     delta_df = pd.DataFrame(delta_data)\n",
    "#     sns.violinplot(x=\"optimizer\", y=\"delta\", data=delta_df, ax=axes[0, 0])\n",
    "#     axes[0, 0].set_title(\"Distribution of Score Changes\")\n",
    "#     axes[0, 0].set_xlabel(\"Optimizer\")\n",
    "#     axes[0, 0].set_ylabel(\"Score Delta\")\n",
    "#     axes[0, 0].axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     # Plot 2: Positive vs negative changes\n",
    "#     pos_neg_data = []\n",
    "#     for optimizer, deltas in all_deltas.items():\n",
    "#         deltas = np.array(deltas)\n",
    "#         pos_pct = np.sum(deltas > 0) / len(deltas) * 100 if len(deltas) > 0 else 0\n",
    "#         neg_pct = np.sum(deltas < 0) / len(deltas) * 100 if len(deltas) > 0 else 0\n",
    "#         zero_pct = np.sum(deltas == 0) / len(deltas) * 100 if len(deltas) > 0 else 0\n",
    "\n",
    "#         pos_neg_data.append({\"optimizer\": optimizer, \"type\": \"Improvement\", \"percentage\": pos_pct})\n",
    "#         pos_neg_data.append({\"optimizer\": optimizer, \"type\": \"Decline\", \"percentage\": neg_pct})\n",
    "#         pos_neg_data.append({\"optimizer\": optimizer, \"type\": \"No Change\", \"percentage\": zero_pct})\n",
    "\n",
    "#     pos_neg_df = pd.DataFrame(pos_neg_data)\n",
    "#     sns.barplot(x=\"optimizer\", y=\"percentage\", hue=\"type\", data=pos_neg_df, ax=axes[0, 1])\n",
    "#     axes[0, 1].set_title(\"Direction of Changes\")\n",
    "#     axes[0, 1].set_xlabel(\"Optimizer\")\n",
    "#     axes[0, 1].set_ylabel(\"Percentage\")\n",
    "#     axes[0, 1].legend(title=\"\")\n",
    "\n",
    "#     # Plot 3: Improvement magnitudes\n",
    "#     improvement_data = []\n",
    "#     for optimizer, deltas in all_deltas.items():\n",
    "#         deltas = np.array(deltas)\n",
    "\n",
    "#         # Get positive deltas only\n",
    "#         pos_deltas = deltas[deltas > 0]\n",
    "\n",
    "#         if len(pos_deltas) > 0:\n",
    "#             small_pct = np.sum(pos_deltas < 0.01) / len(pos_deltas) * 100\n",
    "#             medium_pct = np.sum((pos_deltas >= 0.01) & (pos_deltas < 0.05)) / len(pos_deltas) * 100\n",
    "#             large_pct = np.sum(pos_deltas >= 0.05) / len(pos_deltas) * 100\n",
    "\n",
    "#             improvement_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"type\": \"Small (<1%)\", \"percentage\": small_pct}\n",
    "#             )\n",
    "#             improvement_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"type\": \"Medium (1-5%)\", \"percentage\": medium_pct}\n",
    "#             )\n",
    "#             improvement_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"type\": \"Large (>5%)\", \"percentage\": large_pct}\n",
    "#             )\n",
    "\n",
    "#     improvement_df = pd.DataFrame(improvement_data)\n",
    "#     sns.barplot(x=\"optimizer\", y=\"percentage\", hue=\"type\", data=improvement_df, ax=axes[1, 0])\n",
    "#     axes[1, 0].set_title(\"Magnitude of Improvements\")\n",
    "#     axes[1, 0].set_xlabel(\"Optimizer\")\n",
    "#     axes[1, 0].set_ylabel(\"Percentage\")\n",
    "#     axes[1, 0].legend(title=\"\")\n",
    "\n",
    "#     # Plot 4: Summary statistics\n",
    "#     stats_data = []\n",
    "#     for optimizer, deltas in all_deltas.items():\n",
    "#         deltas = np.array(deltas)\n",
    "\n",
    "#         if len(deltas) > 0:\n",
    "#             stats_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"metric\": \"Max Improvement\", \"value\": np.max(deltas)}\n",
    "#             )\n",
    "#             stats_data.append(\n",
    "#                 {\n",
    "#                     \"optimizer\": optimizer,\n",
    "#                     \"metric\": \"Avg Improvement\",\n",
    "#                     \"value\": np.mean(deltas[deltas > 0]) if any(deltas > 0) else 0,\n",
    "#                 }\n",
    "#             )\n",
    "#             stats_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"metric\": \"Avg Change\", \"value\": np.mean(deltas)}\n",
    "#             )\n",
    "#             stats_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"metric\": \"Median Change\", \"value\": np.median(deltas)}\n",
    "#             )\n",
    "#             stats_data.append(\n",
    "#                 {\"optimizer\": optimizer, \"metric\": \"Volatility\", \"value\": np.std(deltas)}\n",
    "#             )\n",
    "\n",
    "#     stats_df = pd.DataFrame(stats_data)\n",
    "#     sns.barplot(x=\"optimizer\", y=\"value\", hue=\"metric\", data=stats_df, ax=axes[1, 1])\n",
    "#     axes[1, 1].set_title(\"Optimization Strategy Metrics\")\n",
    "#     axes[1, 1].set_xlabel(\"Optimizer\")\n",
    "#     axes[1, 1].set_ylabel(\"Value\")\n",
    "#     axes[1, 1].legend(title=\"\", loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f\"Optimization Strategy Comparison: {dataset} - {model}\", y=1.02, fontsize=16)\n",
    "\n",
    "#     return fig, all_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run\n",
    "# analyze_prompt_diversity(\"gsm8k\", \"llama\", [\"CAPO\", \"EvoPromptGA\"], max_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capo-JzHO-Y9b-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
