sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_evo_hyp --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_EvoPromptGA_hyp --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/EvoPromptGA/hyper/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer EvoPromptGA --n-steps 999 --population-size 10 --n-eval-samples 300 --evoprompt-ga-template standard"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_default/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_us/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 8 --max-n-blocks-eval 10 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_mb/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 30 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_lp/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.01 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_co/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 10 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper/alpha --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.1"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_ps/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 15 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.2"

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_shuffle/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 10 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 5 --max-n-blocks-eval 10 --alpha 0.2 --shuffle-blocks-per-iter"



=== RUNDE 2 ===

sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_us_and_pop_size/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 12 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 8 --max-n-blocks-eval 10 --alpha 0.2"


sbatch --partition=mcml-hgx-a100-80x4 --ntasks=1 --gres=gpu:1 --time=0-02:00:00 --qos=mcml --job-name=benchmark_experiment_sst-5_llama_CAPO_44 --output=logs/%x-%j.out --error=logs/%x-%j.err --wrap "poetry run python scripts/experiment.py --experiment-name benchmark_experiment_sst-5_llama_CAPO_44 --random-seed 42 --budget-per-run 10000000 --output-dir results//sst-5/llama/CAPO/hyper_us_and_pop_size_and_shuffle/ --dataset sst-5 --model vllm-ConfidentialMind/Mistral-Small-24B-Instruct-2501_GPTQ_G128_W4A16_MSE --model-revision 803393813b8fc4046fb663af2e3c56339a5b520b  --max-model-len 1024 --model-storage-path ../models/ --optimizer CAPO --n-steps 999 --population-size 12 --block-size 30 --length-penalty 0.05 --crossovers-per-iter 4 --upper-shots 8 --max-n-blocks-eval 10 --alpha 0.2"